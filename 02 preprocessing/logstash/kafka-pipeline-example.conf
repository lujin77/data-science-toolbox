input {
    kafka {
        zk_connect => "XX.XX.XX.XX:2181"
        group_id => "logstash"
        topic_id => "TOPIC"
        codec => plain
        reset_beginning => false # boolean (optional)， default: false
        consumer_threads => 4  # number (optional)， default: 1
        decorate_events => true # boolean (optional)， default: false
    }

    # for dev test
    #stdin {}
}

filter {

    # 舍弃表头
    if [message] =~ /datetime/ {
        drop {}
    }

    # 以空格切割，以标题组成KV
    ruby {
        init => "@kname = ['datetime', 'id', 'json_str', 'response_time', 'accept_status', 'response_lat', 'response_lng', 'response_distance', 'response_time_length', 'decision_time', 'decision_total_rate', 'decision_result', 'decision_failure_reason', 'decision_msg_snapshot', 'subtract_amount', 'add_price_set', 'response_snapshot', 'is_assigned', 'route_distance', 'route_time_length', 'distance_time_length']"
        code => "
            # 去除中文种的空格，防止切分错误
            log_origin = event.get('message')
            log_filter = log_origin.gsub(/:\"([\w\\\d()-]+\s)+[\w\\\d()-]+\"/) { |matched| matched.split(' ').join('_')}
            log_splited = log_filter.split(' ')
            if log_splited.size != 30
                event['datetime'] = log_splited[0]
                event['_split_log_failure'] = 'split log failed'
            else
                new_event = LogStash::Event.new(Hash[@kname.zip(log_splited)])
                event.append(new_event)
            end
        "
    }

    if [_split_log_failure] {
        mutate {
            gsub => ["datetime", "_", " "]
            remove_field => [ "@version", "@timestamp" ]
        }
        date {
            match => ["datetime", "yyyy-MM-dd HH:mm:ss"]
            locale => "en"
            timezone => "+00:00"
        }
    } else {

        mutate {
          gsub => ["datetime", "_", " "]
          # 需要删除的field
          remove_field => [ "message", "@version", "@timestamp", "host" ]
        }

        date {
          match => ["datetime", "yyyy-MM-dd HH:mm:ss"]
          locale => "en"
          timezone => "+00:00"
        }

        # json解析
        if [json_str] {
          json {
            source => "json_str"
            target => "json_str"
            add_field => {
              "ext_type" => "%{[json_str][type]}"
              "ext_version" => "%{[json_str][version]}"
              "ext_color" => "%{[json_str][color]}"
              "ext_json" => "%{[json_str][inner_json]}"
            }
          }
        } else {
          mutate {
              add_field => {
                "_json_str" => "nil"
              }
          }
        }

        # nested json parse
        if [ext_json] and [ext_json] != "-" {
          json {
            source => "ext_json"
            target => "ext_json"
            add_field => {
              "inner_value" => "%{[ext_json][inner_value]}"
            }
          }
        } else {
          mutate {
            add_field => {
              "inner_value" => "-"
            }
          }
        }

        mutate {
          remove_field => [ "json_str", "ext_json" ]
        }


    }

}

output {
  if [_split_log_failure] or [_jsonparsefailure] {
      #stdout { codec => rubydebug { metadata => true } }
      file {
          path => "out/parsed_log.%{+yyyy}%{+MM}%{+dd}.error"
          codec => line { format => "%{@timestamp} ${datetime} %{message}"}
      }
  } else {
      #stdout { codec => rubydebug { metadata => true } }
      csv {
          path => "out/parsed_log.%{+yyyy}%{+MM}%{+dd}.csv"
          fields => [ 'datetime', 'id', 'type', 'version', 'color', 'inner_value' ]
      }
  }
}
